# Gemini CLI 大语言模型（LLM）调用流程解析

本文档深入剖析了 Gemini CLI 从接收用户输入到调用大语言模型（LLM）并处理其响应的完整内部工作流程。

## 1. 核心组件与分层架构

LLM 的调用流程被清晰地划分到不同的抽象层，每一层都有明确的职责：

1.  **UI 层 (`@google/gemini-cli/src/ui/hooks/useGeminiStream.ts`)**: 
    *   **职责**: 作为“总指挥”，负责编排整个对话回合、管理 UI 状态、处理用户输入、并管理工具调用的完整生命周期（调度、批准、执行、返回结果）。

2.  **客户端层 (`@google/gemini-cli-core/src/core/client.ts`)**: 
    *   **职责**: 作为“大脑”，负责管理聊天会话的业务逻辑。它组装最终发送给模型的请求，实现了如对话历史压缩、IDE 上下文注入、循环检测等高级功能。

3.  **会话层 (`@google/gemini-cli-core/src/core/geminiChat.ts`)**: 
    *   **职责**: 一个轻量级的会话状态持有者，封装了 `@google/genai` 的 `ChatSession` 对象，并最终调用 `ContentGenerator`。

4.  **内容生成层 (`@google/gemini-cli-core/src/core/contentGenerator.ts`)**: 
    *   **职责**: 作为“执行者”，是与 Google API 直接通信的最终接口。它的具体实现根据认证方式（API Key 或 OAuth）而定。

## 2. 请求的生命周期：一次完整的对话回合

以下是一次典型对话回合（Turn）的完整生命周期：

### 步骤 1: 用户输入与预处理

1.  用户在 CLI 界面输入提示并提交。
2.  UI 层的 `useGeminiStream` Hook 调用其核心入口函数 `submitQuery`。
3.  `submitQuery` 内部首先调用 `prepareQueryForGemini` 对用户输入进行预处理。
4.  `prepareQueryForGemini` 会检查输入是否为特殊命令（如 `/help`, `@file`）。
    *   如果是，则由相应的处理器（`handleSlashCommand`, `handleAtCommand`）接管，可能直接返回结果或修改提示内容，而不调用 LLM。
    *   如果不是，则将其作为标准用户提示，准备发送给 LLM。

### 步骤 2: 组装请求 (`GeminiClient`)

`GeminiClient` 负责组装发送给模型的完整上下文。

1.  **历史压缩**: `tryCompressChat` 函数被调用。它会检查当前对话历史的 token 总量。如果超过模型限制的 70%，它会启动一个“元操作”：让 LLM 用一段简短的提示（`getCompressionPrompt`）来**总结**较早的对话历史，然后用这个总结替换掉原来的多轮对话，从而为新内容腾出空间。

2.  **注入上下文**: 
    *   **IDE 上下文**: 如果在 IDE 模式下，`getIdeContextParts` 会被调用，将当前打开的文件、光标位置、选中代码等信息，甚至与上一回合的“状态差异”，作为一个 JSON 对象注入到上下文中。
    *   **系统提示**: `getCoreSystemPrompt` 加载一个核心系统提示，指导模型的行为和角色。
    *   **工具定义**: 从 `ToolRegistry` 中获取所有已注册工具（如 `run_shell_command`, `read_file`）的声明，并将其包含在请求中。

### 步骤 3: 发送请求与事件流处理

1.  所有上下文组装完毕后，`GeminiClient` 调用 `geminiChat.sendMessageStream()`，这会进一步调用 `ContentGenerator`，最终向 Google API 发出 `generateContentStream` 请求。
2.  API 返回一个事件流 (`AsyncIterable<GeminiEvent>`)。
3.  UI 层的 `processGeminiStreamEvents` 函数开始遍历这个事件流，这是一个巨大的 `switch` 语句，用于处理各种类型的事件。

### 步骤 4: 处理模型响应

- **`Content` 事件**: 这是最常见的事件，包含模型生成的文本片段。UI 会将这些片段追加到缓冲区，实时渲染出来，形成“打字机”效果。
- **`Thought` 事件**: 模型在决定下一步行动（尤其是决定是否调用工具）时的“思考”过程。这些思考内容也会被展示给用户，增加了透明度。
- **`ToolCallRequest` 事件**: **这是工具调用的核心**。当模型决定使用一个或多个工具时，会发送此事件。`processGeminiStreamEvents` 会捕获这些请求，并调用 `scheduleToolCalls` 函数，将任务交给工具调度器。
- **`Error` / `Finished` 事件**: 处理 API 错误或标识一轮对话的正常结束。

### 步骤 5: 工具调用（Function Calling）的实现

1.  **调度与批准**: `useReactToolScheduler` Hook 负责管理工具调用的生命周期。
    *   当收到 `scheduleToolCalls` 的调用后，它会将工具任务置于“计划中”。
    *   对于高风险工具（如 `write_file`），它会将状态置于 `awaiting_approval`，并暂停执行，等待用户在 UI 上进行确认。
    *   用户可以通过 `/config set approval_mode yolo` 设置自动批准所有工具调用。

2.  **执行**: 一旦工具被批准，调度器会执行工具对应的本地 TypeScript/JavaScript 函数。

3.  **返回结果**: 
    *   工具执行完毕后，`handleCompletedTools` 函数会被调用。
    *   它将所有工具的输出（stdout, stderr, 或函数返回值）打包成 Gemini API 指定的 `FunctionResponse` 格式。
    *   然后，它会**再次调用 `submitQuery`**，将这些工具的执行结果作为新的“用户”输入，发送回给 LLM。

### 步骤 6: 循环与结束

- LLM 接收到工具的执行结果后，会根据这些信息生成下一步的回复。这个回复可能是最终的答案，也可能是对另一个工具的调用请求。
- 如果是新的工具调用请求，则流程回到**步骤 4**，形成一个“`LLM 思考 -> 工具调用 -> 返回结果 -> LLM 思考`”的循环。
- `LoopDetectionService` 会监控这个循环，如果发现模型在重复调用相同的工具，它会中断流程，防止死循环。
- 当 LLM 认为任务已完成，不再调用工具，并生成了最终的文本回复后，它会发送 `Finished` 事件，整个对话回合结束。

## 3. 错误处理与模型回退

- **API 错误**: `processGeminiStreamEvents` 会捕获 `Error` 事件，并在 UI 上显示格式化后的错误信息。
- **配额用尽 (429 Error)**: `GeminiClient` 中的 `retryWithBackoff` 逻辑会处理临时的 429 错误。如果错误持续存在，`handleFallback` 函数会被调用，它会将当前 `config` 中的模型（如 `gemini-1.5-pro`）自动切换到一个备用模型（如 `gemini-1.5-flash`），并提示用户。这个切换的状态会持续到下一次用户手动发起新提示。
# Gemini CLI IDE 集成与通信流程

本文档深入解析了 Gemini CLI 如何通过其 VS Code 伴侣扩展（Companion Extension）实现与 IDE 的深度集成，以及两者之间通信的底层架构和流程。

## 1. 架构概述

CLI 与 IDE 的集成并非通过直接的进程间通信（IPC）实现，而是通过一个设计精巧的、基于本地 HTTP 服务的**三层架构**来完成。这种解耦的设计使得 CLI 核心逻辑与特定的 IDE 实现无关。

**核心组件**:

1.  **VS Code 伴侣扩展 (`vscode-ide-companion`)**: 
    *   **职责**: 作为“IDE 端点”，直接与 VS Code API 交互。它负责监听 VS Code 的状态变化（如打开文件、光标移动），并执行来自 CLI 的命令（如打开一个 diff 视图）。

2.  **A2A 服务器 (`a2a-server`)**: 
    *   **职责**: 作为“通信桥梁”（Agent-to-Agent Server），是一个在本地运行的轻量级 Express HTTP 服务器。它为 CLI 和 IDE 扩展提供了一个稳定、统一的 RESTful API 通信接口。

3.  **IDE 客户端 (`IdeClient`)**: 
    *   **职责**: 作为“CLI 端点”，位于 `core` 包中。CLI 的其他部分通过调用 `IdeClient` 的方法来与 IDE 通信，而无需关心底层的 HTTP 请求细节。

**通信流程图:**

```
+-----------------------+        +-----------------------+        +-----------------------+
|      Gemini CLI       |        |      A2A Server       |        | VS Code + Extension   |
| (Core / IdeClient)    |        |   (localhost:port)    |        | (vscode-ide-companion)|
+-----------------------+        +-----------------------+        +-----------------------+
           |                             |                              |
           |----- (1) HTTP Request ---->|                              |
           |      (e.g., getContext)     |                              |
           |                             |---- (2) Call Method ---->   |
           |                             |      (e.g., server.getContext()) |
           |                             |                              |--- (3) VS Code API --→
           |                             |                              |   (e.g., get active editor)
           |                             |                              |←-- (4) Return Data ---
           |                             |←--- (5) Return Data ----|   
           |                             |      (e.g., { activeFile: ... }) |
           |←---- (6) HTTP Response ----|                              |
           |      (e.g., 200 OK + JSON)  |                              |

```

## 2. 详细执行步骤

### 步骤 1: 启动与发现

1.  **扩展激活**: 用户安装并启用 `vscode-ide-companion` 扩展后，当 VS Code 启动时，扩展的 `activate` 函数 (`packages/vscode-ide-companion/src/extension.ts`) 被调用。

2.  **启动 A2A 服务器**: `activate` 函数会创建一个 `IDEServer` 实例，并调用其 `start()` 方法。这个方法会**动态地**在本地找一个空闲端口，并启动 `a2a-server` 这个 Express 应用。

3.  **写入端口信息**: 服务器启动后，它会将其监听的端口号等连接信息写入一个位于特定目录（如 `~/.gemini/a2a/`）的 JSON 文件中。

4.  **CLI 发现服务器**: 当 Gemini CLI 主进程启动时，`IdeClient` 会被实例化。它会去检查预定的目录中是否存在 A2A 服务器写入的连接信息文件。如果存在，`IdeClient` 就知道了 A2A 服务器的地址和端口，并与其建立连接。

### 步骤 2: 从 CLI 到 IDE (获取上下文)

这是一个典型的“拉”数据流程，CLI 主动从 IDE 获取信息。

1.  **触发**: `GeminiClient` 在准备向 LLM 发送请求时，会调用 `getIdeContextParts()` 方法。

2.  **读取缓存**: 该方法首先会访问 `ideContextStore`。这是一个简单的内存状态存储，用于缓存从 IDE 获取的上下文，以避免频繁的 HTTP 请求。

3.  **发起请求**: 如果需要更新上下文（或者首次获取），`IdeClient` 会向 A2A 服务器发起一个 HTTP GET 请求，例如 `GET http://localhost:31337/context`。

4.  **服务器处理**: `a2a-server` 接收到请求，其对应的路由处理器被触发。

5.  **调用 VS Code API**: 该处理器会直接调用 VS Code 的 API，例如 `vscode.window.activeTextEditor` 和 `vscode.workspace.textDocuments`，来获取当前活动文件、光标位置、打开的文件列表等信息。

6.  **返回数据**: 服务器将获取到的信息打包成一个 JSON 对象，并通过 HTTP 响应返回给 `IdeClient`。

7.  **更新状态**: `IdeClient` 接收到响应，并用它来更新 `ideContextStore` 中的状态。`GeminiClient` 随后从 `ideContextStore` 中读取到最新的上下文，并将其注入到给 LLM 的提示中。

### 步骤 3: 从 IDE 到 CLI (执行操作)

这是一个典型的“推”操作流程，CLI 命令 IDE 执行一个动作。

1.  **触发**: 当 LLM 返回一个需要修改文件的 `replace` 工具调用时，`CoreToolScheduler` 会将该工具的状态置为 `awaiting_approval`。

2.  **IDE 确认**: 调度器检测到这是一个 `edit` 类型的确认，并且存在 `ideConfirmation` 选项。它不会在终端显示确认提示，而是将这个确认请求通过 `IdeClient` 发送给 A2A 服务器。

3.  **服务器处理**: `a2a-server` 接收到请求，并调用 `DiffManager` 的 `showDiff` 方法。

4.  **显示 Diff 视图**: `DiffManager` 使用 `vscode.diff` 命令，打开一个 VS Code 内置的差异对比视图。左侧是原始文件内容，右侧是 LLM 建议的修改。它还会在编辑器标题栏上添加“接受”和“取消”的按钮。

5.  **用户交互**: 用户在 VS Code 的 UI 中点击“接受”按钮。

6.  **IDE 响应**: 按钮的命令处理器 (`gemini.diff.accept`) 被触发，它会通知 `DiffManager`。

7.  **返回结果**: `DiffManager` 通过一个 Promise 将用户的“接受”决策返回给 `a2a-server`，服务器再通过 HTTP 响应将这个决策返回给 CLI 的 `CoreToolScheduler`。

8.  **CLI 执行**: `CoreToolScheduler` 收到“接受”的响应后，将工具状态切换为 `scheduled`，并最终执行文件写入操作，完成闭环。

## 3. 模型上下文协议 (MCP)

除了上述的自定义 RESTful API，该项目还实现了一个更通用的“模型上下文协议”（Model Context Protocol, MCP）。当 CLI 需要从 IDE 动态发现可用的工具时，就会使用此协议。`ToolRegistry` 中的 `mcpClientManager` 会连接到 A2A 服务器，并请求其提供符合 MCP 规范的工具列表，从而实现了 IDE 向 CLI “注入”新能力的功能。
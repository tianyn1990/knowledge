# Gemini CLI 对话历史压缩流程

本文档深入解析了 Gemini CLI 为解决大语言模型（LLM）上下文窗口限制而设计的核心功能之一：对话历史压缩机制。

## 1. 流程概述

随着对话的进行，上下文（包含历史记录、工具定义、系统提示等）的 token 总量会不断增长。当接近模型的最大上下文窗口时，API 请求会失败。为了支持无限轮次的对话，Gemini CLI 实现了一套智能的、自动化的历史压缩流程。

其核心思想是：**在上下文变得过长时，利用 LLM 自身的能力，将对话中较早的部分进行总结，并用这个凝练后的总结替换掉原来的多轮对话，从而为新的对话“腾出空间”。**

**核心组件**:

- **`GeminiClient` (`packages/core/src/core/client.ts`)**: 压缩流程的主要编排者，其中的 `tryCompressChat` 方法包含了所有核心逻辑。
- **`prompts.ts` (`packages/core/src/core/prompts.ts`)**: 提供了 `getCompressionPrompt` 函数，该函数返回一个专门用于指示 LLM 执行总结任务的系统提示。
- **`tokenLimits.ts` (`packages/core/src/core/tokenLimits.ts`)**: 提供了一个 `tokenLimit` 函数，用于查询不同模型（如 `gemini-1.5-pro`）的确切上下文窗口大小。

## 2. 详细执行步骤

历史压缩流程在每一次用户提交新提示后、向 LLM 发送实际请求之前被触发。

### 步骤 1: 触发条件检查 (`tryCompressChat`)

1.  **入口**: `GeminiClient` 的 `sendMessageStream` 方法在处理用户请求时，会首先调用 `tryCompressChat` 方法。

2.  **计算当前 Token**: 该方法首先会获取当前完整的对话历史，并调用 `contentGenerator.countTokens()` 方法来计算如果立即发送请求，其总 token 数量会是多少（`originalTokenCount`）。

3.  **判断是否需要压缩**: 它将当前 token 数与模型的最大 token 限制进行比较。压缩的触发条件是：

    ```
    originalTokenCount > tokenLimit(model) * 0.7
    ```

    即，当上下文占用了模型总容量的 **70%** 以上时，压缩流程被激活。如果未达到该阈值，则函数直接返回，不执行任何操作。

### 步骤 2: 分割对话历史

一旦压缩被触发，`tryCompressChat` 会将当前的对话历史分割成两部分：

1.  **需要被压缩的部分 (`historyToCompress`)**: 使用 `findIndexAfterFraction` 函数，取对话历史中较早的 **70%** 内容。
2.  **需要被保留的部分 (`historyToKeep`)**: 对话历史中较新的 **30%** 内容。这部分内容将原封不动地保留下来，以确保对话的连贯性。

### 步骤 3: 调用 LLM 进行“自我总结”

这是整个流程中最巧妙的一步：利用模型来压缩模型自己的历史。

1.  **构造压缩请求**: 程序会发起一次**全新的、独立的** `generateContent` API 调用。

2.  **设置特殊提示**: 这次调用的系统提示（`systemInstruction`）被设置为 `getCompressionPrompt()` 的返回值。这个提示会指示模型：“你现在的任务不是回答问题，而是作为一个历史摘要器。请阅读以下对话历史，并生成一个简洁的状态快照（`<state_snapshot>`)，捕捉所有关键信息。”

3.  **发送旧历史**: `historyToCompress`（即需要被压缩的旧历史）被作为普通内容发送给模型。

4.  **获取总结**: LLM 在遵循压缩提示的指导下，返回对旧历史的总结文本（`summary`）。

### 步骤 4: 重建聊天会话

1.  **创建新历史**: 程序获取到 `summary` 后，会构建一个新的对话历史数组。这个新历史的结构是：
    *   一段引导文本，告诉模型这是一个总结（`{ role: 'user', parts: [{ text: summary }] }`）。
    *   一段模型的确认（`{ role: 'model', parts: [{ text: 'Got it. Thanks...' }] }`）。
    *   之前被保留的、较新的 30% 对话历史 (`historyToKeep`)。

2.  **重启会话**: `GeminiClient` 调用 `this.startChat()` 方法，并传入这个全新的、更短的对话历史，从而创建一个新的 `GeminiChat` 实例。旧的、过长的会话实例被丢弃。

3.  **健全性检查**: 在替换会话之前，程序会计算新历史的 token 总量（`newTokenCount`），并确保它小于原始的 `originalTokenCount`。如果因为某些原因，总结后的 token 反而更多，则放弃本次压缩，保留原始历史，以避免负面效果。

### 步骤 5: 继续正常流程

1.  历史压缩完成后，`tryCompressChat` 方法返回。
2.  `sendMessageStream` 方法继续执行，但此时它操作的是一个拥有全新、更短历史的 `GeminiChat` 实例。
3.  它将用户当前输入的提示追加到这个新历史的末尾，然后向 Google API 发送请求。

## 3. 结论

Gemini CLI 的对话历史压缩是一个完全自动化、对用户透明的流程。它通过巧妙地利用 LLM 自身的总结能力，在不丢失关键上下文信息的前提下，有效地解决了长对话场景中的 token 限制问题，理论上实现了“无限长度”的对话。